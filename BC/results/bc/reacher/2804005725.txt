Policy train: iteration: 500, policy_loss: 0.002534
Policy train: iteration: 1000, policy_loss: 0.002830
Policy train: iteration: 1500, policy_loss: 0.002650
Policy train: iteration: 2000, policy_loss: 0.002492
Policy train: iteration: 2500, policy_loss: 0.002099
Policy train: iteration: 3000, policy_loss: 0.001979
Policy train: iteration: 3500, policy_loss: 0.002238
Policy train: iteration: 4000, policy_loss: 0.002457
Policy train: iteration: 4500, policy_loss: 0.001834
Policy train: iteration: 5000, policy_loss: 0.001764
Policy train: iteration: 5500, policy_loss: 0.002208
Policy train: iteration: 6000, policy_loss: 0.002149

Background Trial: 1, reward: -102.06975441474106
Background Trial: 2, reward: -20.7029888292062
Background Trial: 3, reward: -23.463190365200568
Background Trial: 4, reward: -116.03571307432394
Background Trial: 5, reward: -16.62234688118596
Background Trial: 6, reward: -17.99684707483068
Background Trial: 7, reward: -18.19716309903334
Background Trial: 8, reward: -102.25047590641034
Background Trial: 9, reward: -20.711549832669384
Iteration: 1, average_reward: -48.67222549751127

Policy train: iteration: 500, policy_loss: 0.001979
Policy train: iteration: 1000, policy_loss: 0.001859
Policy train: iteration: 1500, policy_loss: 0.002129
Policy train: iteration: 2000, policy_loss: 0.001496
Policy train: iteration: 2500, policy_loss: 0.001892
Policy train: iteration: 3000, policy_loss: 0.001910
Policy train: iteration: 3500, policy_loss: 0.001874
Policy train: iteration: 4000, policy_loss: 0.002146
Policy train: iteration: 4500, policy_loss: 0.002007
Policy train: iteration: 5000, policy_loss: 0.001872
Policy train: iteration: 5500, policy_loss: 0.002176
Policy train: iteration: 6000, policy_loss: 0.002009

Background Trial: 1, reward: -105.7431889663509
Background Trial: 2, reward: -143.3095407034101
Background Trial: 3, reward: -76.43996912723581
Background Trial: 4, reward: -95.93836666647194
Background Trial: 5, reward: -67.30758828339984
Background Trial: 6, reward: -43.97883936706567
Background Trial: 7, reward: -40.83139676984642
Background Trial: 8, reward: -45.9155736272213
Background Trial: 9, reward: -40.08887989998075
Iteration: 2, average_reward: -73.28370482344252

Policy train: iteration: 500, policy_loss: 0.001406
Policy train: iteration: 1000, policy_loss: 0.001758
Policy train: iteration: 1500, policy_loss: 0.001615
Policy train: iteration: 2000, policy_loss: 0.001924
Policy train: iteration: 2500, policy_loss: 0.001615
Policy train: iteration: 3000, policy_loss: 0.001902
Policy train: iteration: 3500, policy_loss: 0.002267
Policy train: iteration: 4000, policy_loss: 0.002074
Policy train: iteration: 4500, policy_loss: 0.001486
Policy train: iteration: 5000, policy_loss: 0.001871
Policy train: iteration: 5500, policy_loss: 0.001804
Policy train: iteration: 6000, policy_loss: 0.001743

Background Trial: 1, reward: -96.05352352124781
Background Trial: 2, reward: -106.4527651951236
Background Trial: 3, reward: -37.72099229076177
Background Trial: 4, reward: -99.80057583955069
Background Trial: 5, reward: -100.00866987977277
Background Trial: 6, reward: -101.44417683735405
Background Trial: 7, reward: -105.8407200372939
Background Trial: 8, reward: -105.23455023985593
Background Trial: 9, reward: -99.96191748373451
Iteration: 3, average_reward: -94.72421014718833

Policy train: iteration: 500, policy_loss: 0.001456
Policy train: iteration: 1000, policy_loss: 0.001855
Policy train: iteration: 1500, policy_loss: 0.002009
Policy train: iteration: 2000, policy_loss: 0.001932
Policy train: iteration: 2500, policy_loss: 0.001787
Policy train: iteration: 3000, policy_loss: 0.002006
Policy train: iteration: 3500, policy_loss: 0.001651
Policy train: iteration: 4000, policy_loss: 0.001301
Policy train: iteration: 4500, policy_loss: 0.001639
Policy train: iteration: 5000, policy_loss: 0.001835
Policy train: iteration: 5500, policy_loss: 0.001872
Policy train: iteration: 6000, policy_loss: 0.001945

Background Trial: 1, reward: -97.19186727952342
Background Trial: 2, reward: -82.39569776380222
Background Trial: 3, reward: -94.70471023362336
Background Trial: 4, reward: -94.55401721057714
Background Trial: 5, reward: -96.811563509797
Background Trial: 6, reward: -97.3874697592802
Background Trial: 7, reward: -98.29148708081179
Background Trial: 8, reward: -105.92708621110896
Background Trial: 9, reward: -88.12408583637844
Iteration: 4, average_reward: -95.04310943165584

Policy train: iteration: 500, policy_loss: 0.001978
Policy train: iteration: 1000, policy_loss: 0.001641
Policy train: iteration: 1500, policy_loss: 0.001832
Policy train: iteration: 2000, policy_loss: 0.001846
Policy train: iteration: 2500, policy_loss: 0.001697
Policy train: iteration: 3000, policy_loss: 0.002283
Policy train: iteration: 3500, policy_loss: 0.001681
Policy train: iteration: 4000, policy_loss: 0.001731
Policy train: iteration: 4500, policy_loss: 0.002099
Policy train: iteration: 5000, policy_loss: 0.001497
Policy train: iteration: 5500, policy_loss: 0.001878
Policy train: iteration: 6000, policy_loss: 0.001480

Background Trial: 1, reward: -90.95623857355466
Background Trial: 2, reward: -99.96199571924882
Background Trial: 3, reward: -92.0108118765336
Background Trial: 4, reward: -111.93790573884722
Background Trial: 5, reward: -94.82824395535303
Background Trial: 6, reward: -104.1369274101587
Background Trial: 7, reward: -73.45222742851516
Background Trial: 8, reward: -112.66101948310353
Background Trial: 9, reward: -107.214113725163
Iteration: 5, average_reward: -98.57327599005308

Policy train: iteration: 500, policy_loss: 0.001770
Policy train: iteration: 1000, policy_loss: 0.002064
