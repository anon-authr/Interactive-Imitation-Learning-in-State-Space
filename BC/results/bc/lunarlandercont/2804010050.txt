Policy train: iteration: 500, policy_loss: 0.350333
Policy train: iteration: 1000, policy_loss: 0.284028
Policy train: iteration: 1500, policy_loss: 0.288921
Policy train: iteration: 2000, policy_loss: 0.252956
Policy train: iteration: 2500, policy_loss: 0.233501
Policy train: iteration: 3000, policy_loss: 0.236185
Policy train: iteration: 3500, policy_loss: 0.239928
Policy train: iteration: 4000, policy_loss: 0.227173
Policy train: iteration: 4500, policy_loss: 0.211773
Policy train: iteration: 5000, policy_loss: 0.214296
Policy train: iteration: 5500, policy_loss: 0.257397
Policy train: iteration: 6000, policy_loss: 0.225893
Policy train: iteration: 6500, policy_loss: 0.202731
Policy train: iteration: 7000, policy_loss: 0.203820
Policy train: iteration: 7500, policy_loss: 0.220604
Policy train: iteration: 8000, policy_loss: 0.202119
Policy train: iteration: 8500, policy_loss: 0.263606
Policy train: iteration: 9000, policy_loss: 0.217863

Background Trial: 1, reward: -43.06679932337474
Background Trial: 2, reward: -61.36688380928602
Background Trial: 3, reward: -72.3086079503538
Background Trial: 4, reward: 16.70620808503753
Background Trial: 5, reward: 24.42518568788732
Background Trial: 6, reward: 0.2803666524376638
Background Trial: 7, reward: 235.87973438868613
Background Trial: 8, reward: -203.62667586119107
Background Trial: 9, reward: 23.76159812666114
Iteration: 1, average_reward: -8.812874889277314

Policy train: iteration: 500, policy_loss: 0.195137
Policy train: iteration: 1000, policy_loss: 0.205158
Policy train: iteration: 1500, policy_loss: 0.196616
Policy train: iteration: 2000, policy_loss: 0.204549
Policy train: iteration: 2500, policy_loss: 0.199790
Policy train: iteration: 3000, policy_loss: 0.180349
Policy train: iteration: 3500, policy_loss: 0.176622
Policy train: iteration: 4000, policy_loss: 0.204074
Policy train: iteration: 4500, policy_loss: 0.192319
Policy train: iteration: 5000, policy_loss: 0.158906
Policy train: iteration: 5500, policy_loss: 0.166085
Policy train: iteration: 6000, policy_loss: 0.157194
Policy train: iteration: 6500, policy_loss: 0.195988
Policy train: iteration: 7000, policy_loss: 0.156010
Policy train: iteration: 7500, policy_loss: 0.184528
Policy train: iteration: 8000, policy_loss: 0.152557
Policy train: iteration: 8500, policy_loss: 0.135572
Policy train: iteration: 9000, policy_loss: 0.146544

Background Trial: 1, reward: -14.554576609365114
Background Trial: 2, reward: -4.147463832462606
Background Trial: 3, reward: -38.832923642914025
Background Trial: 4, reward: -50.40023032046394
Background Trial: 5, reward: -18.415164137808446
Background Trial: 6, reward: 14.400984158550187
Background Trial: 7, reward: -298.0941090376457
Background Trial: 8, reward: -72.10659572055249
Background Trial: 9, reward: -162.93029077017462
Iteration: 2, average_reward: -71.67559665698187

Policy train: iteration: 500, policy_loss: 0.181533
Policy train: iteration: 1000, policy_loss: 0.168523
Policy train: iteration: 1500, policy_loss: 0.151820
Policy train: iteration: 2000, policy_loss: 0.169324
Policy train: iteration: 2500, policy_loss: 0.142647
Policy train: iteration: 3000, policy_loss: 0.172654
Policy train: iteration: 3500, policy_loss: 0.191435
Policy train: iteration: 4000, policy_loss: 0.149474
Policy train: iteration: 4500, policy_loss: 0.167094
Policy train: iteration: 5000, policy_loss: 0.179697
Policy train: iteration: 5500, policy_loss: 0.141825
Policy train: iteration: 6000, policy_loss: 0.177125
Policy train: iteration: 6500, policy_loss: 0.139942
Policy train: iteration: 7000, policy_loss: 0.168960
Policy train: iteration: 7500, policy_loss: 0.141250
Policy train: iteration: 8000, policy_loss: 0.155980
Policy train: iteration: 8500, policy_loss: 0.134948
Policy train: iteration: 9000, policy_loss: 0.142513

Background Trial: 1, reward: -37.15302727269021
Background Trial: 2, reward: -58.40504481367918
Background Trial: 3, reward: -40.18817248725297
Background Trial: 4, reward: -57.73918682852896
Background Trial: 5, reward: -35.299928234078976
Background Trial: 6, reward: -81.40486101345743
Background Trial: 7, reward: -49.15607004690169
Background Trial: 8, reward: -64.64721504210266
Background Trial: 9, reward: 7.534831271752452
Iteration: 3, average_reward: -46.27318605188218

Policy train: iteration: 500, policy_loss: 0.165912
Policy train: iteration: 1000, policy_loss: 0.185145
Policy train: iteration: 1500, policy_loss: 0.146393
Policy train: iteration: 2000, policy_loss: 0.147085
Policy train: iteration: 2500, policy_loss: 0.131987
Policy train: iteration: 3000, policy_loss: 0.155527
Policy train: iteration: 3500, policy_loss: 0.168532
Policy train: iteration: 4000, policy_loss: 0.133085
Policy train: iteration: 4500, policy_loss: 0.141578
Policy train: iteration: 5000, policy_loss: 0.136282
Policy train: iteration: 5500, policy_loss: 0.149825
Policy train: iteration: 6000, policy_loss: 0.193219
Policy train: iteration: 6500, policy_loss: 0.139180
Policy train: iteration: 7000, policy_loss: 0.130109
Policy train: iteration: 7500, policy_loss: 0.128351
Policy train: iteration: 8000, policy_loss: 0.130968
Policy train: iteration: 8500, policy_loss: 0.120615
Policy train: iteration: 9000, policy_loss: 0.140386

Background Trial: 1, reward: -50.51172607187547
Background Trial: 2, reward: -46.58077018774609
Background Trial: 3, reward: 26.96006851403716
Background Trial: 4, reward: 268.72823593053977
Background Trial: 5, reward: 270.25531205956725
Background Trial: 6, reward: -217.67578820087192
Background Trial: 7, reward: -74.2942892037072
Background Trial: 8, reward: -55.13449753312297
Background Trial: 9, reward: -30.600840060272972
Iteration: 4, average_reward: 10.127300582949731

Policy train: iteration: 500, policy_loss: 0.165785
Policy train: iteration: 1000, policy_loss: 0.141873
Policy train: iteration: 1500, policy_loss: 0.136696
Policy train: iteration: 2000, policy_loss: 0.142980
Policy train: iteration: 2500, policy_loss: 0.145785
Policy train: iteration: 3000, policy_loss: 0.149955
Policy train: iteration: 3500, policy_loss: 0.115269
Policy train: iteration: 4000, policy_loss: 0.178232
Policy train: iteration: 4500, policy_loss: 0.166930
Policy train: iteration: 5000, policy_loss: 0.141855
Policy train: iteration: 5500, policy_loss: 0.137853
Policy train: iteration: 6000, policy_loss: 0.154591
Policy train: iteration: 6500, policy_loss: 0.130157
Policy train: iteration: 7000, policy_loss: 0.108949
Policy train: iteration: 7500, policy_loss: 0.123605
Policy train: iteration: 8000, policy_loss: 0.166566
Policy train: iteration: 8500, policy_loss: 0.161780
Policy train: iteration: 9000, policy_loss: 0.130206

Background Trial: 1, reward: -51.83212079835809
Background Trial: 2, reward: -71.29417245517969
Background Trial: 3, reward: -0.4262388189891624
Background Trial: 4, reward: -324.9534650089262
Background Trial: 5, reward: -22.976175135017414
Background Trial: 6, reward: -3.4864510065780507
Background Trial: 7, reward: -39.3226318844232
Background Trial: 8, reward: -77.16756313138181
Background Trial: 9, reward: -42.931282307343736
Iteration: 5, average_reward: -70.48778894957748

Policy train: iteration: 500, policy_loss: 0.135474
Policy train: iteration: 1000, policy_loss: 0.154099
Policy train: iteration: 1500, policy_loss: 0.140557
Policy train: iteration: 2000, policy_loss: 0.134733
Policy train: iteration: 2500, policy_loss: 0.198900
Policy train: iteration: 3000, policy_loss: 0.105123
Policy train: iteration: 3500, policy_loss: 0.160231
Policy train: iteration: 4000, policy_loss: 0.166244
Policy train: iteration: 4500, policy_loss: 0.125108
Policy train: iteration: 5000, policy_loss: 0.134839
Policy train: iteration: 5500, policy_loss: 0.149900
Policy train: iteration: 6000, policy_loss: 0.140803
Policy train: iteration: 6500, policy_loss: 0.156192
Policy train: iteration: 7000, policy_loss: 0.101693
Policy train: iteration: 7500, policy_loss: 0.156344
Policy train: iteration: 8000, policy_loss: 0.132815
Policy train: iteration: 8500, policy_loss: 0.144888
Policy train: iteration: 9000, policy_loss: 0.138273

Background Trial: 1, reward: -179.02766079844986
Background Trial: 2, reward: -74.16456602071753
Background Trial: 3, reward: 28.871620742362865
Background Trial: 4, reward: -35.30399644111186
Background Trial: 5, reward: -63.4176334603813
Background Trial: 6, reward: -40.99949533347102
Background Trial: 7, reward: -1.00772861750373
Background Trial: 8, reward: -9.700275561541105
Background Trial: 9, reward: -74.7446687171329
Iteration: 6, average_reward: -49.94382268977183

Policy train: iteration: 500, policy_loss: 0.109845
Policy train: iteration: 1000, policy_loss: 0.095842
Policy train: iteration: 1500, policy_loss: 0.144542
Policy train: iteration: 2000, policy_loss: 0.102610
Policy train: iteration: 2500, policy_loss: 0.137637
Policy train: iteration: 3000, policy_loss: 0.121463
Policy train: iteration: 3500, policy_loss: 0.124585
Policy train: iteration: 4000, policy_loss: 0.111792
Policy train: iteration: 4500, policy_loss: 0.150866
Policy train: iteration: 5000, policy_loss: 0.086062
Policy train: iteration: 5500, policy_loss: 0.130648
Policy train: iteration: 6000, policy_loss: 0.124881
Policy train: iteration: 6500, policy_loss: 0.165019
Policy train: iteration: 7000, policy_loss: 0.160758
Policy train: iteration: 7500, policy_loss: 0.117008
Policy train: iteration: 8000, policy_loss: 0.147637
Policy train: iteration: 8500, policy_loss: 0.142214
Policy train: iteration: 9000, policy_loss: 0.123019

Background Trial: 1, reward: -78.07231227265098
Background Trial: 2, reward: -47.23117466890457
Background Trial: 3, reward: 16.19495527648371
Background Trial: 4, reward: 14.10637401813321
Background Trial: 5, reward: -51.1156709583173
Background Trial: 6, reward: -246.85853015614876
Background Trial: 7, reward: -59.116535180085016
Background Trial: 8, reward: 15.312148129145243
Background Trial: 9, reward: -235.49289126629523
Iteration: 7, average_reward: -74.69707078651552

Policy train: iteration: 500, policy_loss: 0.142037
Policy train: iteration: 1000, policy_loss: 0.126187
Policy train: iteration: 1500, policy_loss: 0.118276
Policy train: iteration: 2000, policy_loss: 0.175323
Policy train: iteration: 2500, policy_loss: 0.140194
Policy train: iteration: 3000, policy_loss: 0.093365
Policy train: iteration: 3500, policy_loss: 0.149316
Policy train: iteration: 4000, policy_loss: 0.125533
Policy train: iteration: 4500, policy_loss: 0.104881
Policy train: iteration: 5000, policy_loss: 0.124734
Policy train: iteration: 5500, policy_loss: 0.132225
Policy train: iteration: 6000, policy_loss: 0.152625
Policy train: iteration: 6500, policy_loss: 0.091071
Policy train: iteration: 7000, policy_loss: 0.111900
Policy train: iteration: 7500, policy_loss: 0.101344
Policy train: iteration: 8000, policy_loss: 0.109184
Policy train: iteration: 8500, policy_loss: 0.123008
Policy train: iteration: 9000, policy_loss: 0.144250

Background Trial: 1, reward: -32.73122565340664
Background Trial: 2, reward: -53.03237380222401
Background Trial: 3, reward: 243.15978242001205
Background Trial: 4, reward: -71.32792169518167
Background Trial: 5, reward: -358.6187513161954
Background Trial: 6, reward: -39.31170523401245
Background Trial: 7, reward: -227.33326199531348
Background Trial: 8, reward: 23.400021392476177
Background Trial: 9, reward: -42.54738992742557
Iteration: 8, average_reward: -62.03809175680788

Policy train: iteration: 500, policy_loss: 0.133394
Policy train: iteration: 1000, policy_loss: 0.128927
Policy train: iteration: 1500, policy_loss: 0.092266
Policy train: iteration: 2000, policy_loss: 0.140086
Policy train: iteration: 2500, policy_loss: 0.117782
Policy train: iteration: 3000, policy_loss: 0.128227
Policy train: iteration: 3500, policy_loss: 0.128071
Policy train: iteration: 4000, policy_loss: 0.128754
Policy train: iteration: 4500, policy_loss: 0.124930
Policy train: iteration: 5000, policy_loss: 0.103434
Policy train: iteration: 5500, policy_loss: 0.112324
Policy train: iteration: 6000, policy_loss: 0.117330
Policy train: iteration: 6500, policy_loss: 0.129701
Policy train: iteration: 7000, policy_loss: 0.117248
Policy train: iteration: 7500, policy_loss: 0.138151
Policy train: iteration: 8000, policy_loss: 0.115316
Policy train: iteration: 8500, policy_loss: 0.131665
Policy train: iteration: 9000, policy_loss: 0.115079

Background Trial: 1, reward: -212.3719364366284
Background Trial: 2, reward: -22.275011298429973
Background Trial: 3, reward: -499.0469559462789
Background Trial: 4, reward: -38.250894136120024
Background Trial: 5, reward: -19.448127664696827
Background Trial: 6, reward: -51.36540958219212
Background Trial: 7, reward: 23.733139288035645
Background Trial: 8, reward: -82.47470828911054
Background Trial: 9, reward: -14.846762919810786
Iteration: 9, average_reward: -101.81629633169243

Policy train: iteration: 500, policy_loss: 0.112554
Policy train: iteration: 1000, policy_loss: 0.119814
Policy train: iteration: 1500, policy_loss: 0.122034
Policy train: iteration: 2000, policy_loss: 0.134632
Policy train: iteration: 2500, policy_loss: 0.131207
Policy train: iteration: 3000, policy_loss: 0.116419
Policy train: iteration: 3500, policy_loss: 0.129509
Policy train: iteration: 4000, policy_loss: 0.105874
Policy train: iteration: 4500, policy_loss: 0.123539
Policy train: iteration: 5000, policy_loss: 0.132087
Policy train: iteration: 5500, policy_loss: 0.089429
Policy train: iteration: 6000, policy_loss: 0.134260
Policy train: iteration: 6500, policy_loss: 0.129162
Policy train: iteration: 7000, policy_loss: 0.125858
Policy train: iteration: 7500, policy_loss: 0.103512
Policy train: iteration: 8000, policy_loss: 0.143443
Policy train: iteration: 8500, policy_loss: 0.146061
Policy train: iteration: 9000, policy_loss: 0.113386

Background Trial: 1, reward: -49.29225290746464
Background Trial: 2, reward: 210.68946228243757
Background Trial: 3, reward: -79.91504536110273
Background Trial: 4, reward: -57.2452402874753
Background Trial: 5, reward: -242.43522406495646
Background Trial: 6, reward: -40.84088994332008
Background Trial: 7, reward: 37.28641380958203
Background Trial: 8, reward: -40.09052076576161
Background Trial: 9, reward: -51.182818953937684
Iteration: 10, average_reward: -34.780679576888765

Policy train: iteration: 500, policy_loss: 0.157167
Policy train: iteration: 1000, policy_loss: 0.133184
Policy train: iteration: 1500, policy_loss: 0.134854
Policy train: iteration: 2000, policy_loss: 0.110997
Policy train: iteration: 2500, policy_loss: 0.134301
Policy train: iteration: 3000, policy_loss: 0.100945
Policy train: iteration: 3500, policy_loss: 0.126039
Policy train: iteration: 4000, policy_loss: 0.093124
Policy train: iteration: 4500, policy_loss: 0.112444
Policy train: iteration: 5000, policy_loss: 0.108798
Policy train: iteration: 5500, policy_loss: 0.157662
Policy train: iteration: 6000, policy_loss: 0.133319
Policy train: iteration: 6500, policy_loss: 0.095052
Policy train: iteration: 7000, policy_loss: 0.140080
Policy train: iteration: 7500, policy_loss: 0.166615
Policy train: iteration: 8000, policy_loss: 0.133777
Policy train: iteration: 8500, policy_loss: 0.095816
Policy train: iteration: 9000, policy_loss: 0.130319

Background Trial: 1, reward: -59.06816910297298
Background Trial: 2, reward: -28.56176802957785
Background Trial: 3, reward: -26.0299455956812
Background Trial: 4, reward: 6.47730193285993
Background Trial: 5, reward: -59.934368607343885
Background Trial: 6, reward: -39.79498798470803
Background Trial: 7, reward: -172.23888738962586
Background Trial: 8, reward: -10.812326053905934
Background Trial: 9, reward: -44.561885699305535
Iteration: 11, average_reward: -48.28055961447348

Policy train: iteration: 500, policy_loss: 0.132820
Policy train: iteration: 1000, policy_loss: 0.137259
Policy train: iteration: 1500, policy_loss: 0.141071
Policy train: iteration: 2000, policy_loss: 0.146796
Policy train: iteration: 2500, policy_loss: 0.116188
Policy train: iteration: 3000, policy_loss: 0.114309
Policy train: iteration: 3500, policy_loss: 0.109011
Policy train: iteration: 4000, policy_loss: 0.158855
Policy train: iteration: 4500, policy_loss: 0.085511
Policy train: iteration: 5000, policy_loss: 0.120108
Policy train: iteration: 5500, policy_loss: 0.130670
Policy train: iteration: 6000, policy_loss: 0.122054
Policy train: iteration: 6500, policy_loss: 0.094423
Policy train: iteration: 7000, policy_loss: 0.123916
Policy train: iteration: 7500, policy_loss: 0.118148
Policy train: iteration: 8000, policy_loss: 0.094336
Policy train: iteration: 8500, policy_loss: 0.112900
Policy train: iteration: 9000, policy_loss: 0.125560

Background Trial: 1, reward: -50.325999918232924
Background Trial: 2, reward: -16.073616146012313
Background Trial: 3, reward: -172.8543179707159
Background Trial: 4, reward: -289.71596042989046
Background Trial: 5, reward: -46.56476014822877
Background Trial: 6, reward: -61.74636149485689
Background Trial: 7, reward: -161.63190182798118
Background Trial: 8, reward: -84.41128471175622
Background Trial: 9, reward: -20.732225224973845
Iteration: 12, average_reward: -100.45071420807204

Policy train: iteration: 500, policy_loss: 0.105638
Policy train: iteration: 1000, policy_loss: 0.127626
Policy train: iteration: 1500, policy_loss: 0.124816
Policy train: iteration: 2000, policy_loss: 0.143125
Policy train: iteration: 2500, policy_loss: 0.154326
Policy train: iteration: 3000, policy_loss: 0.115860
Policy train: iteration: 3500, policy_loss: 0.134757
Policy train: iteration: 4000, policy_loss: 0.103469
Policy train: iteration: 4500, policy_loss: 0.110357
Policy train: iteration: 5000, policy_loss: 0.116465
Policy train: iteration: 5500, policy_loss: 0.107587
Policy train: iteration: 6000, policy_loss: 0.104286
Policy train: iteration: 6500, policy_loss: 0.100663
Policy train: iteration: 7000, policy_loss: 0.090838
Policy train: iteration: 7500, policy_loss: 0.105962
Policy train: iteration: 8000, policy_loss: 0.090798
Policy train: iteration: 8500, policy_loss: 0.130905
Policy train: iteration: 9000, policy_loss: 0.107202

Background Trial: 1, reward: -312.0203665539993
Background Trial: 2, reward: -57.2554676433288
Background Trial: 3, reward: -2.633846622577906
Background Trial: 4, reward: -233.44332826757898
Background Trial: 5, reward: -218.67308142975637
Background Trial: 6, reward: -48.20490512086176
Background Trial: 7, reward: -108.67510883263509
Background Trial: 8, reward: -17.29618970910441
Background Trial: 9, reward: -46.92846939681539
Iteration: 13, average_reward: -116.12564039740644

Policy train: iteration: 500, policy_loss: 0.112332
Policy train: iteration: 1000, policy_loss: 0.128005
Policy train: iteration: 1500, policy_loss: 0.112078
Policy train: iteration: 2000, policy_loss: 0.092791
Policy train: iteration: 2500, policy_loss: 0.123827
Policy train: iteration: 3000, policy_loss: 0.100908
Policy train: iteration: 3500, policy_loss: 0.114830
Policy train: iteration: 4000, policy_loss: 0.123791
Policy train: iteration: 4500, policy_loss: 0.099305
Policy train: iteration: 5000, policy_loss: 0.141386
Policy train: iteration: 5500, policy_loss: 0.092593
Policy train: iteration: 6000, policy_loss: 0.131973
Policy train: iteration: 6500, policy_loss: 0.080216
Policy train: iteration: 7000, policy_loss: 0.092520
Policy train: iteration: 7500, policy_loss: 0.112136
Policy train: iteration: 8000, policy_loss: 0.131196
Policy train: iteration: 8500, policy_loss: 0.073952
Policy train: iteration: 9000, policy_loss: 0.097427

Background Trial: 1, reward: -4.359725577406195
Background Trial: 2, reward: -42.651299244719375
Background Trial: 3, reward: -14.336877302333178
Background Trial: 4, reward: -52.27046188354909
Background Trial: 5, reward: -50.53282647091534
Background Trial: 6, reward: -56.16880301772068
Background Trial: 7, reward: -53.10993485519816
Background Trial: 8, reward: 10.350693375786761
Background Trial: 9, reward: -61.8434626929571
Iteration: 14, average_reward: -36.102521963223595

Policy train: iteration: 500, policy_loss: 0.126439
Policy train: iteration: 1000, policy_loss: 0.127004
Policy train: iteration: 1500, policy_loss: 0.105494
Policy train: iteration: 2000, policy_loss: 0.115075
Policy train: iteration: 2500, policy_loss: 0.092063
Policy train: iteration: 3000, policy_loss: 0.101695
Policy train: iteration: 3500, policy_loss: 0.088425
Policy train: iteration: 4000, policy_loss: 0.084251
Policy train: iteration: 4500, policy_loss: 0.096261
Policy train: iteration: 5000, policy_loss: 0.099685
Policy train: iteration: 5500, policy_loss: 0.094214
Policy train: iteration: 6000, policy_loss: 0.140442
Policy train: iteration: 6500, policy_loss: 0.127089
Policy train: iteration: 7000, policy_loss: 0.106027
Policy train: iteration: 7500, policy_loss: 0.123687
Policy train: iteration: 8000, policy_loss: 0.132108
Policy train: iteration: 8500, policy_loss: 0.105410
Policy train: iteration: 9000, policy_loss: 0.102782

Background Trial: 1, reward: -81.37355341619246
Background Trial: 2, reward: -27.4531464253202
Background Trial: 3, reward: 13.310653176631831
Background Trial: 4, reward: -36.30561261320362
Background Trial: 5, reward: -49.72725036975713
Background Trial: 6, reward: -124.97387090067622
Background Trial: 7, reward: -21.33497894584272
Background Trial: 8, reward: -37.01369348402412
Background Trial: 9, reward: -70.97886824109884
Iteration: 15, average_reward: -48.42781346883149

Policy train: iteration: 500, policy_loss: 0.137236
Policy train: iteration: 1000, policy_loss: 0.108791
Policy train: iteration: 1500, policy_loss: 0.071323
Policy train: iteration: 2000, policy_loss: 0.119714
Policy train: iteration: 2500, policy_loss: 0.111884
Policy train: iteration: 3000, policy_loss: 0.121095
Policy train: iteration: 3500, policy_loss: 0.115535
Policy train: iteration: 4000, policy_loss: 0.104991
Policy train: iteration: 4500, policy_loss: 0.095219
Policy train: iteration: 5000, policy_loss: 0.132262
Policy train: iteration: 5500, policy_loss: 0.111821
Policy train: iteration: 6000, policy_loss: 0.078422
Policy train: iteration: 6500, policy_loss: 0.108666
Policy train: iteration: 7000, policy_loss: 0.137202
Policy train: iteration: 7500, policy_loss: 0.123247
Policy train: iteration: 8000, policy_loss: 0.098393
Policy train: iteration: 8500, policy_loss: 0.101645
Policy train: iteration: 9000, policy_loss: 0.128547

Background Trial: 1, reward: -30.063802651400366
Background Trial: 2, reward: -7.177871559700293
Background Trial: 3, reward: -50.031732883056236
Background Trial: 4, reward: -243.51774988469498
Background Trial: 5, reward: -65.60111722059304
Background Trial: 6, reward: -10.366354689367029
Background Trial: 7, reward: 17.485058857284045
Background Trial: 8, reward: -53.20600844544126
Background Trial: 9, reward: -54.024835067283334
Iteration: 16, average_reward: -55.1671570604725

Policy train: iteration: 500, policy_loss: 0.109884
Policy train: iteration: 1000, policy_loss: 0.112928
Policy train: iteration: 1500, policy_loss: 0.114746
Policy train: iteration: 2000, policy_loss: 0.101792
Policy train: iteration: 2500, policy_loss: 0.104003
Policy train: iteration: 3000, policy_loss: 0.104461
Policy train: iteration: 3500, policy_loss: 0.099061
Policy train: iteration: 4000, policy_loss: 0.146251
Policy train: iteration: 4500, policy_loss: 0.119093
Policy train: iteration: 5000, policy_loss: 0.101214
Policy train: iteration: 5500, policy_loss: 0.104554
Policy train: iteration: 6000, policy_loss: 0.080806
Policy train: iteration: 6500, policy_loss: 0.125448
Policy train: iteration: 7000, policy_loss: 0.110140
Policy train: iteration: 7500, policy_loss: 0.095665
Policy train: iteration: 8000, policy_loss: 0.092466
Policy train: iteration: 8500, policy_loss: 0.106683
Policy train: iteration: 9000, policy_loss: 0.126540

Background Trial: 1, reward: -42.21559525799843
Background Trial: 2, reward: -336.08057957595884
Background Trial: 3, reward: -68.12449853788979
Background Trial: 4, reward: -328.61160100495346
Background Trial: 5, reward: -21.537126080880384
Background Trial: 6, reward: 27.48731419453371
Background Trial: 7, reward: -193.76952603340862
Background Trial: 8, reward: 29.976112825842108
Background Trial: 9, reward: -6.373366397895623
Iteration: 17, average_reward: -104.36098509651215

Policy train: iteration: 500, policy_loss: 0.106597
Policy train: iteration: 1000, policy_loss: 0.099407
Policy train: iteration: 1500, policy_loss: 0.087239
Policy train: iteration: 2000, policy_loss: 0.089195
Policy train: iteration: 2500, policy_loss: 0.104773
Policy train: iteration: 3000, policy_loss: 0.135615
Policy train: iteration: 3500, policy_loss: 0.103133
Policy train: iteration: 4000, policy_loss: 0.152493
Policy train: iteration: 4500, policy_loss: 0.099791
Policy train: iteration: 5000, policy_loss: 0.098046
Policy train: iteration: 5500, policy_loss: 0.114740
Policy train: iteration: 6000, policy_loss: 0.100492
Policy train: iteration: 6500, policy_loss: 0.095727
Policy train: iteration: 7000, policy_loss: 0.097474
Policy train: iteration: 7500, policy_loss: 0.091450
Policy train: iteration: 8000, policy_loss: 0.098785
Policy train: iteration: 8500, policy_loss: 0.098478
Policy train: iteration: 9000, policy_loss: 0.108280

Background Trial: 1, reward: -70.25955080850211
Background Trial: 2, reward: -538.2030377572698
Background Trial: 3, reward: -48.07372196659564
Background Trial: 4, reward: -116.0700659032058
Background Trial: 5, reward: -70.32068569289318
Background Trial: 6, reward: -57.793240309873056
Background Trial: 7, reward: -21.675419552910455
Background Trial: 8, reward: -108.89324331907034
Background Trial: 9, reward: -257.28162781409503
Iteration: 18, average_reward: -143.17451034715725

Policy train: iteration: 500, policy_loss: 0.083941
Policy train: iteration: 1000, policy_loss: 0.092402
Policy train: iteration: 1500, policy_loss: 0.116264
Policy train: iteration: 2000, policy_loss: 0.093823
Policy train: iteration: 2500, policy_loss: 0.099957
Policy train: iteration: 3000, policy_loss: 0.125242
Policy train: iteration: 3500, policy_loss: 0.090492
Policy train: iteration: 4000, policy_loss: 0.101322
Policy train: iteration: 4500, policy_loss: 0.091068
Policy train: iteration: 5000, policy_loss: 0.089778
Policy train: iteration: 5500, policy_loss: 0.062457
Policy train: iteration: 6000, policy_loss: 0.113155
Policy train: iteration: 6500, policy_loss: 0.111540
Policy train: iteration: 7000, policy_loss: 0.067939
Policy train: iteration: 7500, policy_loss: 0.111946
Policy train: iteration: 8000, policy_loss: 0.130342
Policy train: iteration: 8500, policy_loss: 0.101159
Policy train: iteration: 9000, policy_loss: 0.099380

Background Trial: 1, reward: 254.19898068484298
Background Trial: 2, reward: -44.50558515363548
Background Trial: 3, reward: -290.5280201890092
Background Trial: 4, reward: -7.598795512364191
Background Trial: 5, reward: -70.06640507736196
Background Trial: 6, reward: -98.5901021808341
Background Trial: 7, reward: -48.96908722801404
Background Trial: 8, reward: -25.11452670601504
Background Trial: 9, reward: 246.59728313885816
Iteration: 19, average_reward: -9.397362024836989

Policy train: iteration: 500, policy_loss: 0.151419
Policy train: iteration: 1000, policy_loss: 0.110064
Policy train: iteration: 1500, policy_loss: 0.109688
Policy train: iteration: 2000, policy_loss: 0.104176
Policy train: iteration: 2500, policy_loss: 0.101383
Policy train: iteration: 3000, policy_loss: 0.082774
Policy train: iteration: 3500, policy_loss: 0.088393
Policy train: iteration: 4000, policy_loss: 0.083219
Policy train: iteration: 4500, policy_loss: 0.086484
Policy train: iteration: 5000, policy_loss: 0.088841
Policy train: iteration: 5500, policy_loss: 0.115131
Policy train: iteration: 6000, policy_loss: 0.058960
Policy train: iteration: 6500, policy_loss: 0.103149
Policy train: iteration: 7000, policy_loss: 0.089878
Policy train: iteration: 7500, policy_loss: 0.116104
Policy train: iteration: 8000, policy_loss: 0.109215
Policy train: iteration: 8500, policy_loss: 0.085749
Policy train: iteration: 9000, policy_loss: 0.116127

Background Trial: 1, reward: -58.47890026415192
Background Trial: 2, reward: -263.63656555786986
Background Trial: 3, reward: 0.6455674029878509
Background Trial: 4, reward: -163.96695818189824
Background Trial: 5, reward: -59.593398536769776
Background Trial: 6, reward: -228.13947523491922
Background Trial: 7, reward: -43.74763951806438
Background Trial: 8, reward: -28.757111344350122
Background Trial: 9, reward: -6.575398784935615
Iteration: 20, average_reward: -94.69443111333015

Policy train: iteration: 500, policy_loss: 0.093773
Policy train: iteration: 1000, policy_loss: 0.114420
Policy train: iteration: 1500, policy_loss: 0.098482
Policy train: iteration: 2000, policy_loss: 0.119220
Policy train: iteration: 2500, policy_loss: 0.106971
Policy train: iteration: 3000, policy_loss: 0.095583
Policy train: iteration: 3500, policy_loss: 0.071742
Policy train: iteration: 4000, policy_loss: 0.112351
Policy train: iteration: 4500, policy_loss: 0.079214
Policy train: iteration: 5000, policy_loss: 0.078421
Policy train: iteration: 5500, policy_loss: 0.118755
Policy train: iteration: 6000, policy_loss: 0.098627
Policy train: iteration: 6500, policy_loss: 0.081168
Policy train: iteration: 7000, policy_loss: 0.100236
Policy train: iteration: 7500, policy_loss: 0.103741
Policy train: iteration: 8000, policy_loss: 0.105503
Policy train: iteration: 8500, policy_loss: 0.084756
Policy train: iteration: 9000, policy_loss: 0.071284

Background Trial: 1, reward: -43.62297759200479
Background Trial: 2, reward: -63.85170040994673
Background Trial: 3, reward: -67.82851378193789
Background Trial: 4, reward: -18.45615063257584
Background Trial: 5, reward: -1.986634469119906
Background Trial: 6, reward: -22.02631153543784
Background Trial: 7, reward: -83.2350410521067
Background Trial: 8, reward: -212.23862237250069
Background Trial: 9, reward: -79.66133884955953
Iteration: 21, average_reward: -65.8785878550211

Policy train: iteration: 500, policy_loss: 0.108865
Policy train: iteration: 1000, policy_loss: 0.086052
Policy train: iteration: 1500, policy_loss: 0.082766
Policy train: iteration: 2000, policy_loss: 0.095502
Policy train: iteration: 2500, policy_loss: 0.098919
Policy train: iteration: 3000, policy_loss: 0.066075
Policy train: iteration: 3500, policy_loss: 0.093435
Policy train: iteration: 4000, policy_loss: 0.083104
Policy train: iteration: 4500, policy_loss: 0.101122
Policy train: iteration: 5000, policy_loss: 0.109923
Policy train: iteration: 5500, policy_loss: 0.100099
Policy train: iteration: 6000, policy_loss: 0.122138
Policy train: iteration: 6500, policy_loss: 0.096219
Policy train: iteration: 7000, policy_loss: 0.112631
Policy train: iteration: 7500, policy_loss: 0.070343
Policy train: iteration: 8000, policy_loss: 0.060052
Policy train: iteration: 8500, policy_loss: 0.094971
Policy train: iteration: 9000, policy_loss: 0.089211

Background Trial: 1, reward: 3.4808280328145287
Background Trial: 2, reward: -18.212765548286683
Background Trial: 3, reward: -66.15889033852969
Background Trial: 4, reward: 8.047724821604191
Background Trial: 5, reward: -45.835686463514854
Background Trial: 6, reward: -226.18719685173983
Background Trial: 7, reward: -78.33963961209388
Background Trial: 8, reward: -18.10339408641778
Background Trial: 9, reward: -15.588920828747504
Iteration: 22, average_reward: -50.76643787499017

Policy train: iteration: 500, policy_loss: 0.102431
Policy train: iteration: 1000, policy_loss: 0.075354
Policy train: iteration: 1500, policy_loss: 0.101224
Policy train: iteration: 2000, policy_loss: 0.129543
Policy train: iteration: 2500, policy_loss: 0.111890
Policy train: iteration: 3000, policy_loss: 0.104748
Policy train: iteration: 3500, policy_loss: 0.106129
Policy train: iteration: 4000, policy_loss: 0.092851
Policy train: iteration: 4500, policy_loss: 0.089065
Policy train: iteration: 5000, policy_loss: 0.122601
Policy train: iteration: 5500, policy_loss: 0.058342
Policy train: iteration: 6000, policy_loss: 0.084575
Policy train: iteration: 6500, policy_loss: 0.092773
Policy train: iteration: 7000, policy_loss: 0.113545
Policy train: iteration: 7500, policy_loss: 0.073737
Policy train: iteration: 8000, policy_loss: 0.100618
Policy train: iteration: 8500, policy_loss: 0.089917
Policy train: iteration: 9000, policy_loss: 0.127104

Background Trial: 1, reward: -565.6241735983517
Background Trial: 2, reward: -48.389148757631986
Background Trial: 3, reward: -52.49628016967125
Background Trial: 4, reward: -71.81527341338806
Background Trial: 5, reward: -207.1306472180087
Background Trial: 6, reward: -39.632436015684334
Background Trial: 7, reward: -57.860443881661936
Background Trial: 8, reward: -36.10284300789003
Background Trial: 9, reward: -341.14120164517317
Iteration: 23, average_reward: -157.7991608563846

Policy train: iteration: 500, policy_loss: 0.077767
Policy train: iteration: 1000, policy_loss: 0.095780
Policy train: iteration: 1500, policy_loss: 0.095116
Policy train: iteration: 2000, policy_loss: 0.118572
Policy train: iteration: 2500, policy_loss: 0.103854
Policy train: iteration: 3000, policy_loss: 0.116573
Policy train: iteration: 3500, policy_loss: 0.073906
Policy train: iteration: 4000, policy_loss: 0.104321
Policy train: iteration: 4500, policy_loss: 0.089172
Policy train: iteration: 5000, policy_loss: 0.059179
Policy train: iteration: 5500, policy_loss: 0.073143
Policy train: iteration: 6000, policy_loss: 0.065942
Policy train: iteration: 6500, policy_loss: 0.100346
Policy train: iteration: 7000, policy_loss: 0.087150
Policy train: iteration: 7500, policy_loss: 0.088290
Policy train: iteration: 8000, policy_loss: 0.096972
Policy train: iteration: 8500, policy_loss: 0.080901
Policy train: iteration: 9000, policy_loss: 0.089185

Background Trial: 1, reward: -255.7090166599911
Background Trial: 2, reward: -47.02286806632385
Background Trial: 3, reward: -99.61528054112263
Background Trial: 4, reward: -46.29236984976178
Background Trial: 5, reward: -11.601138520660498
Background Trial: 6, reward: -80.50687808040655
Background Trial: 7, reward: -68.17934589664063
Background Trial: 8, reward: 3.0198483039085318
Background Trial: 9, reward: -72.12696702947972
Iteration: 24, average_reward: -75.3371129267198

Policy train: iteration: 500, policy_loss: 0.086899
Policy train: iteration: 1000, policy_loss: 0.081287
Policy train: iteration: 1500, policy_loss: 0.110958
Policy train: iteration: 2000, policy_loss: 0.101773
Policy train: iteration: 2500, policy_loss: 0.123448
Policy train: iteration: 3000, policy_loss: 0.081377
Policy train: iteration: 3500, policy_loss: 0.102798
Policy train: iteration: 4000, policy_loss: 0.116051
Policy train: iteration: 4500, policy_loss: 0.105538
Policy train: iteration: 5000, policy_loss: 0.075923
Policy train: iteration: 5500, policy_loss: 0.081613
Policy train: iteration: 6000, policy_loss: 0.087782
Policy train: iteration: 6500, policy_loss: 0.073590
Policy train: iteration: 7000, policy_loss: 0.078587
Policy train: iteration: 7500, policy_loss: 0.131220
Policy train: iteration: 8000, policy_loss: 0.072588
Policy train: iteration: 8500, policy_loss: 0.099933
Policy train: iteration: 9000, policy_loss: 0.088375

Background Trial: 1, reward: -74.80148228131333
Background Trial: 2, reward: -23.687480435296408
Background Trial: 3, reward: -262.64504530189083
Background Trial: 4, reward: -91.40508901732444
Background Trial: 5, reward: 7.386635087959377
Background Trial: 6, reward: -34.52652189846302
Background Trial: 7, reward: -218.99620105293081
Background Trial: 8, reward: -70.40355582456591
Background Trial: 9, reward: -76.64238153451396
Iteration: 25, average_reward: -93.96901358425993

Policy train: iteration: 500, policy_loss: 0.071535
Policy train: iteration: 1000, policy_loss: 0.096959
Policy train: iteration: 1500, policy_loss: 0.077280
Policy train: iteration: 2000, policy_loss: 0.127173
Policy train: iteration: 2500, policy_loss: 0.076667
Policy train: iteration: 3000, policy_loss: 0.078089
Policy train: iteration: 3500, policy_loss: 0.113904
Policy train: iteration: 4000, policy_loss: 0.091815
Policy train: iteration: 4500, policy_loss: 0.089737
Policy train: iteration: 5000, policy_loss: 0.067046
Policy train: iteration: 5500, policy_loss: 0.129719
Policy train: iteration: 6000, policy_loss: 0.103770
Policy train: iteration: 6500, policy_loss: 0.095238
Policy train: iteration: 7000, policy_loss: 0.098415
Policy train: iteration: 7500, policy_loss: 0.107909
Policy train: iteration: 8000, policy_loss: 0.103023
Policy train: iteration: 8500, policy_loss: 0.102543
Policy train: iteration: 9000, policy_loss: 0.072593

Background Trial: 1, reward: 20.58523198672816
Background Trial: 2, reward: -160.41248912678242
Background Trial: 3, reward: -41.63546570213807
Background Trial: 4, reward: -66.54764648624095
Background Trial: 5, reward: -43.29576755322796
Background Trial: 6, reward: -52.42078912941726
Background Trial: 7, reward: -25.44066772505832
Background Trial: 8, reward: -232.23219525008392
Background Trial: 9, reward: -38.247921757714465
Iteration: 26, average_reward: -71.07196786043725

Policy train: iteration: 500, policy_loss: 0.094847
Policy train: iteration: 1000, policy_loss: 0.098064
Policy train: iteration: 1500, policy_loss: 0.078069
